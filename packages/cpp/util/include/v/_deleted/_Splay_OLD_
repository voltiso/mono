#pragma once
#include <type_traits>
#include <v/_/_>

#include "v/HashMap"
#include "v/Object"
#include "v/Pool"
#include "v/Singleton"
#include "v/Splay"
#include "v/array"
#include "v/context"
#include "v/getParameter/VALUE"
#include "v/memory/PAGE_SIZE"
#include "v/parameter"
#include <cstddef>

// ! experimental - unused, 6x slower than malloc, does not ever return memory
// ! to the system

// ! currently not even correct - incorrectly assumed splay trees can never have
// ! large heights, and the Splay class buffer can be InPlaceOnly

namespace VOLTISO_NAMESPACE::allocator::splay {
// struct Defaults {
//   // parent allocator
//   using Allocator = Malloc;
//   static constexpr auto PAGE_SIZE = memory::PAGE_SIZE;
//   using Brand = void;
// };
// using DefaultOptions = Options<Defaults>;

template <class Final, class Parameters> class Custom;
} // namespace VOLTISO_NAMESPACE::allocator::splay

//

namespace VOLTISO_NAMESPACE::allocator::splay::_ {
template <Size PAGE_SIZE> auto roundUpToFullPages(Size numBytes) {
	return (numBytes + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);
	//
}
/** @internal used to self-allocate memory for our internal structures, in order
 * to avoid dependency on self */
template <class Final, class Parameters> class SelfAllocator {
public:
	using Handle = void *;

	static constexpr auto PAGE_SIZE =
	  getParameter::VALUE<parameter::PAGE_SIZE, Parameters>;

	using Allocator = getParameter::Type<parameter::Allocator, Parameters>;

public:
	void *allocateBytes(Size numBytes) {
		// LOG(INFO) << "SelfAllocator::allocateBytes(" << numBytes << ")";
		DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);

		if (
		  this->spareBlocks.numItems &&
		  this->spareBlocks.first().numBytes >= numBytes) { // ! likely? unlikely?
			// LOG(INFO) << "USE SPARE BLOCK DIRECTLY";
			if (this->spareBlocks.first().numBytes != numBytes) [[likely]] {
				auto ptr = this->spareBlocks.first().ptr;
				reinterpret_cast<std::uintptr_t &>(this->spareBlocks.first().ptr) +=
				  numBytes;
				this->spareBlocks.first().numBytes -= numBytes;
				return ptr;
			} else [[unlikely]] {
				auto ptr = this->spareBlocks.first().ptr;
				this->spareBlocks.pop();
				return ptr;
			}
		}

		auto hNode = _alloc()
		               .splay(_alloc().root)
		               .lowerBound(numBytes << (sizeof(Size) * 8 / 2));

		if (hNode) [[likely]] {
			auto entry = _alloc().splay(_alloc().root).erase(hNode);
			DCHECK_GE(entry.numBytes, numBytes);
			// LOG(INFO) << "begins[" << entry.memory.value << "].erase()";
			_alloc().begins[entry.memory.value].erase();
			_alloc().ends[entry.memory.value + entry.numBytes].erase();
			this->_add(entry.memory.value + numBytes, entry.numBytes - numBytes);
			return entry.memory;
		} else [[unlikely]] {
			// LOG(INFO) << "SelfAllocator: allocate full page";
			auto newNumBytes = roundUpToFullPages<PAGE_SIZE>(numBytes);
			auto memory = Allocator::instance().allocateBytes(newNumBytes);
			this->_add(memory.value + numBytes, newNumBytes - numBytes);
			return memory;
		}
	}

public:
	void *reallocateBytes(void *handle, Size numBytes, Size newNumBytes) {
		// LOG(INFO) << "SelfAllocator::reallocateBytes(" << handle << ", " <<
		// numBytes
		//           << ", " << newNumBytes << ")";

		// don't optimize for this branch; shrinking allocations is not done when
		// time is critical
		if (newNumBytes <= numBytes) [[unlikely]] {
			auto pLeftover = (std::uintptr_t)handle + newNumBytes;
			if (newNumBytes != numBytes) [[likely]] {
				auto leftoverSize = numBytes - newNumBytes;
				_add(pLeftover, leftoverSize);
			}
			return handle;
		}

		// auto pAdditional = (std::uintptr_t)handle + numBytes;
		// auto aAdditionalBegin = _alloc().begins[pAdditional];

		// // optimize for fast in-place realloc
		// if (aAdditionalBegin.exists) [[likely]] {
		//   auto aNode = _alloc().nodes[aAdditionalBegin];
		//   auto numAdditionalBytes = newNumBytes - numBytes;
		//   if (aNode->entry.numBytes >= numAdditionalBytes) [[likely]] {
		//     // realloc in place possible
		//     auto entry = _alloc().splay(_alloc().root).erase(aNode);

		//     // remove from begins and ends
		//     aAdditionalBegin.erase();
		//     _alloc().ends[entry.memory.value + entry.numBytes].erase();

		//     // shrink left numAdditionalBytes
		//     if (aNode->entry.numBytes != numAdditionalBytes) [[likely]] {
		//       DCHECK_GT(aNode->entry.numBytes, numAdditionalBytes);
		//       auto pLeftover = (std::uintptr_t)handle + newNumBytes;
		//       auto leftoverSize = aNode->entry.numBytes - numAdditionalBytes;
		//       _add(pLeftover, leftoverSize);
		//     } else [[unlikely]] {
		//       // whole block consumed
		//     }
		//     return handle;
		//   }
		// }

		auto newHandle = allocateBytes(newNumBytes);
		::memcpy(newHandle, handle, numBytes);
		this->freeBytes(handle, numBytes);
		return newHandle;
	}

public:
	void freeBytes(void *ptr, Size numBytes) {
		// LOG(INFO) << "SelfAllocator::freeBytes(" << ptr << ", " << numBytes <<
		// ")";
		this->_add(ptr, numBytes);
	}

public:
	void *operator()(void *ptr) { return ptr; }

public:
	// friend singleton;
	// friend singleton::perThread;
	SelfAllocator() = default;

private:
	friend Custom<Final, Parameters>;
	static constexpr auto &instance() {
		// todo: store in SplayAllocator
		return Singleton<SelfAllocator>::instance();
		// return singleton::perThread::instance<SelfAllocator>();
	}

private:
	auto &_alloc() {
		// return context::get<SelfAllocator>();
		return Custom<Final, Parameters>::instance();
	}

private:
	struct SpareBlock {
		void *ptr;
		Size numBytes;
	};
	using SpareBlocks = DynamicArray<SpareBlock>::template WithInPlaceOnly<100>;
	SpareBlocks spareBlocks;

	void _add(std::uintptr_t memory, Size numBytes) {
		_add(reinterpret_cast<void *>(memory), numBytes);
	}

	void _add(void *memory, Size numBytes) {
		// LOG(INFO) << "Adding spare block " << numBytes << " bytes at " << memory;
		DCHECK_LT(spareBlocks.numItems, spareBlocks.NUM_SLOTS);
		spareBlocks.push({memory, numBytes});
	}

	void _cleanup() {
		// if (spareBlocks.numItems) {
		//   LOG(INFO) << "Cleaning up " << spareBlocks.numItems << " spare blocks";
		// }
		while (spareBlocks.numItems) [[unlikely]] {
			auto block = spareBlocks.pop();
			// LOG(INFO) << "Registering " << block.numBytes << " spare bytes at "
			//           << block.ptr;
			_alloc().freeBytes(
			  typename Custom<Final, Parameters>::Handle(block.ptr), block.numBytes);
		}
	}
}; // class SelfAllocator

template <class Final, class Parameters> struct SplayEntry {
	const Size key;
	const Size numBytes;

	using Allocator = getParameter::Type<parameter::Allocator, Parameters>;
	const Allocator::Handle memory;

	SplayEntry(Size numBytes, Allocator::Handle memory)
	    : key(_key(numBytes, memory)), numBytes(numBytes), memory(memory) {}

	SplayEntry(const SplayEntry &) = default;

	SplayEntry &operator=(const SplayEntry &other) {
		static_assert(std::is_trivially_destructible_v<SplayEntry>);
		new (this) SplayEntry(other);
		return *this;
	}

	auto &value() const { return memory; }

private:
	static constexpr auto _key(Size numBytes, Allocator::Handle memory) {
		constexpr auto BITS = sizeof(Size) * 8 / 2;
		constexpr auto MASK = (Size(1) << BITS) - 1;
		DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);
		DCHECK_EQ(memory.value % alignof(std::max_align_t), 0);
		auto key = numBytes / alignof(std::max_align_t);
		DCHECK_LE(key, MASK);
		auto memoryHash = (memory.value & MASK) / alignof(std::max_align_t);
		auto hash = key ^ memoryHash;
		auto result = (key << BITS) | hash;
		return result;
	}
};

template <class Final, class Parameters>
using Pool =
  Pool<int>::template WithAllocator<SelfAllocator<Final, Parameters>>;

template <class Final, class Parameters> struct SplayNode {
	SplayEntry<Final, Parameters> entry;
	using Handle =
	  typename Pool<Final, Parameters>::Handle::template WithBrand<void>;
	Array<Handle, 2> children = {Handle::INVALID, Handle::INVALID};

	// SplayNode &operator=(const SplayNode &other) {
	//   memcpy(this, &other, sizeof(SplayNode));
	//   return *this;
	// }
};

// using A = GetValue<SplayNode<int, std::tuple<>>>;
// using A = GetValue<SplayEntry<int, std::tuple<>>>;

// static_assert(std::is_same_v<A, void>);

} // namespace VOLTISO_NAMESPACE::allocator::splay::_

namespace std {
template <class Final, class Parameters>
ostream &operator<<(
  ostream &os,
  const VOLTISO_NAMESPACE::allocator::splay::_::SplayEntry<Final, Parameters>
    &entry) {
	return os << "{numBytes: " << entry.numBytes << ", memory: " << entry.memory
	          << "}";
}

template <class Final, class Parameters>
ostream &operator<<(
  ostream &os,
  const VOLTISO_NAMESPACE::allocator::splay::_::SplayNode<Final, Parameters>
    &node) {
	return os << "{entry: " << node.entry << ", children: " << node.children
	          << "}";
}
} // namespace std

//

namespace VOLTISO_NAMESPACE::allocator::splay {
template <class Final, class Parameters> class Custom : public Object<Final> {
private:
	using Self = Custom;

public:
	// parent allocator
	using Allocator = getParameter::Type<parameter::Allocator, Parameters>;

	static constexpr auto PAGE_SIZE =
	  getParameter::VALUE<parameter::PAGE_SIZE, Parameters>;

	using Handle = Allocator::Handle ::template WithBrand<Self>;

private:
	friend _::SelfAllocator<Final, Parameters>;
	using AllocatorHandle = Allocator::Handle;

	context::Guard<_::SelfAllocator<Final, Parameters>> splaySelfGuard =
	  context::Guard(_::SelfAllocator<Final, Parameters>::instance());

private:
	using Pool = _::Pool<Final, Parameters>;
	using SplayEntry = _::SplayEntry<Final, Parameters>;
	using SplayNode = _::SplayNode<Final, Parameters>;
	using Nodes = Pool ::template WithItem<SplayNode>;
	Nodes nodes;
	Nodes::Handle root = Nodes::Handle::INVALID;

private:
	// static_assert(std::is_same_v<typename Nodes::Item, int>);
	Splay<Nodes> splay = Splay<Nodes>(this->nodes);

	static constexpr Size BUCKET_IN_PLACE = 1;
	using Edges =
	  HashMap<std::uintptr_t, typename Nodes::Handle>::template WithAllocator<
	    _::SelfAllocator<Final, Parameters>>;
	// ::template BUCKET_IN_PLACE_<
	//     BUCKET_IN_PLACE>;
	Edges begins, ends;

private:
	friend Singleton<Self>;
	friend Singleton<Final>;
	// friend singleton::perThread;
	Custom() = default;

public:
	static constexpr auto &instance() {
		// return context::get<typename Options::Final>();
		return Singleton<Self>::instance();
		// return singleton::perThread::instance<Self>();
	}

public:
	Handle allocateBytes(const Size numBytes) {
		// LOG(INFO) << "Build::allocateBytes(" << numBytes << ")";
		DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);
		// numBytes = divCeil(numBytes, alignof(std::max_align_t)) *
		//            alignof(std::max_align_t);
		auto hNode = splay(root).lowerBound(numBytes);

		if (!hNode) [[unlikely]] {
			auto newNumBytes = _::roundUpToFullPages<PAGE_SIZE>(numBytes);
			auto memory = Allocator::instance().allocateBytes(newNumBytes);
			_add(memory, newNumBytes);
			hNode = splay(root).lowerBound(numBytes);
		}

		auto aNode = nodes[hNode];
		auto result = aNode->entry.memory;
		DCHECK_GE(aNode->entry.numBytes, numBytes);
		if (aNode->entry.numBytes > numBytes) [[likely]] {
			// Split the node
			begins[aNode->entry.memory.value].erase();

			splay(root).unlink(hNode);
			aNode->entry = {
			  aNode->entry.numBytes - numBytes,
			  AllocatorHandle(aNode->entry.memory.value + numBytes)};
			splay(root).link(hNode);

			begins[aNode->entry.memory.value].insert(hNode);
		} else [[unlikely]] {
			DCHECK_EQ(aNode->entry.numBytes, numBytes);
			begins[aNode->entry.memory.value].erase();
			ends[aNode->entry.memory.value + aNode->entry.numBytes].erase();
			splay(root).erase(hNode);
		}

		DCHECK_EQ(
		  (_::SelfAllocator<Final, Parameters>::instance().spareBlocks.numItems),
		  0);

		return Handle(result.value);
	}

	Handle
	reallocateBytes(const Handle &handle, Size numBytes, Size newNumBytes) {
		// LOG(INFO) << "Build::reallocateBytes(" << handle << ", " << numBytes <<
		// ", "
		//           << newNumBytes << ")";
		// don't optimize for this branch; shrinking allocations is not done when
		// time is critical
		if (newNumBytes <= numBytes) [[unlikely]] {
			auto pLeftover = (std::uintptr_t)handle.value + newNumBytes;
			if (newNumBytes != numBytes) [[likely]] {
				auto leftoverSize = numBytes - newNumBytes;
				_add(AllocatorHandle(pLeftover), leftoverSize);
			}
			DCHECK_EQ(
			  (_::SelfAllocator<Final, Parameters>::instance().spareBlocks.numItems),
			  0);
			return handle;
		}

		auto pAdditional = (std::uintptr_t)handle.value + numBytes;
		auto &aAdditionalBegin = begins[pAdditional];

		// optimize for fast in-place realloc
		if (aAdditionalBegin.exists) [[likely]] {
			auto aNode = nodes[aAdditionalBegin];
			auto numAdditionalBytes = newNumBytes - numBytes;
			if (aNode->entry.numBytes >= numAdditionalBytes) [[likely]] {
				// shrink left numAdditionalBytes
				aAdditionalBegin.erase(); // remove from begins
				auto pLeftover = (std::uintptr_t)handle.value + newNumBytes;
				if (aNode->entry.numBytes != numAdditionalBytes) [[likely]] {
					DCHECK_GT(aNode->entry.numBytes, numAdditionalBytes);
					begins[pLeftover].insert(aNode);
					splay[root].unlink(aNode);
					aNode->entry.numBytes -= numAdditionalBytes;
					splay[root].link(aNode);
				} else [[unlikely]] {
					// whole block consumed
					splay[root].erase(aNode);
				}
				DCHECK_EQ(
				  (_::SelfAllocator<Final, Parameters>::instance()
				     .spareBlocks.numItems),
				  0);
				return handle;
			}
		}

		auto newHandle = allocateBytes(newNumBytes);
		::memcpy(newHandle, handle, numBytes);
		this->freeBytes(handle, numBytes);
		DCHECK_EQ(
		  (_::SelfAllocator<Final, Parameters>::instance().spareBlocks.numItems),
		  0);
		return newHandle;
	}

	void freeBytes(const Handle &handle, Size numBytes) {
		// LOG(INFO) << "Build::freeBytes(" << handle << ", " << numBytes << ")";
		DCHECK_GT(numBytes, 0);
		_add(AllocatorHandle(handle.value), numBytes);
		DCHECK_EQ(
		  (_::SelfAllocator<Final, Parameters>::instance().spareBlocks.numItems),
		  0);
	}

	void *operator()(const Handle &handle) { return handle; }

	const void *operator()(const Handle &handle) const {
		return const_cast<Self &>(*this)(handle);
	}

private:
	// add new block of memory, merging with adjacent blocks if possible
	// adds at most 1 entry to `begins`
	// adds at most 1 entry to `ends`
	// adds at most 1 entry to `nodes`
	void _add(Allocator::Handle memory, Size numBytes) {
		DCHECK_NE(memory.value, 0);
		DCHECK_GT(numBytes, 0);
		DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);

		// _selfGrow();

		auto pNewBlock = (std::uintptr_t)memory.value;
		auto aPrevEnd = ends[pNewBlock];
		if (aPrevEnd.exists) [[unlikely]] {
			// have previous block
			auto aPrevNode = nodes[aPrevEnd];
			aPrevEnd.erase();
			// LOG(INFO) << splay(root);
			splay(root).unlink(aPrevNode);
			auto prevNodeNumBytes = aPrevNode->entry.numBytes;
			prevNodeNumBytes += numBytes;
			auto pNewBlockEnd = (std::uintptr_t)memory.value + numBytes;
			auto aNextBegin = begins[pNewBlockEnd];
			if (aNextBegin.exists) [[unlikely]] {
				// also have next block
				auto aNextNode = nodes[aNextBegin];
				prevNodeNumBytes += aNextNode->entry.numBytes;
				splay(root).erase(aNextNode);
				aNextBegin.erase();
			} else [[likely]] {
				// have previous, but no next block
				ends[pNewBlockEnd].insert(aPrevNode);
			}
			aPrevNode->entry = {prevNodeNumBytes, aPrevNode->entry.memory};
			splay(root).link(aPrevNode);
		} else [[likely]] {
			// no previous block
			auto pNewBlockEnd = (std::uintptr_t)memory.value + numBytes;
			auto aNextBegin = begins[pNewBlockEnd];
			if (aNextBegin.exists) [[unlikely]] {
				// have next block
				auto aNextNode = nodes[aNextBegin];
				begins[aNextNode->entry.memory.value].erase();
				splay(root).unlink(aNextNode);
				aNextNode->entry = {aNextNode->entry.numBytes + numBytes, memory};
				splay(root).link(aNextNode);
				begins[pNewBlock].insert(aNextNode);
			} else [[likely]] {
				// no previous, no next block
				auto aNode =
				  this->nodes.insert(SplayNode{SplayEntry{numBytes, memory}});
				// auto newNode = splay(root).insert(SplayEntry{numBytes, memory});
				// LOG(INFO) << "begins[" << pNewBlock << "].insert(" << aNode << ")";
				ends[pNewBlockEnd].insert(aNode); // has to be first
				begins[pNewBlock].insert(aNode);
				splay(root).link(aNode);
			}
		}
		_::SelfAllocator<Final, Parameters>::instance()._cleanup();
	} // _add
}; // class Custom
} // namespace VOLTISO_NAMESPACE::allocator::splay

// VOLTISO_OBJECT_FINAL(allocator::splay)

namespace VOLTISO_NAMESPACE::allocator {
// using Splay = splay::Final<splay::DefaultOptions>;

class Splay : public splay::Custom<Splay, std::tuple<>> {
	using Base = splay::Custom<Splay, std::tuple<>>;

public:
	using Base::Base;
};

// inline auto g_splayGuard =
// v::context::Guard(v::allocator::Splay::instance());

} // namespace VOLTISO_NAMESPACE::allocator
