#pragma once
#include <type_traits>
#include <voltiso/_>

#include "voltiso/Array"
#include "voltiso/HashMap"
#include "voltiso/Object"
#include "voltiso/Pool"
#include "voltiso/Splay"
#include "voltiso/memory/PAGE_SIZE"

#include <cstddef>

// ! experimental - unused, 6x slower than malloc, does not ever return memory
// ! to the system

namespace VOLTISO_NAMESPACE::allocator::splay {
struct Defaults {
  // parent allocator
  using Allocator = Malloc;
  static constexpr auto PAGE_SIZE = memory::PAGE_SIZE;
  using Brand = void;
};
using DefaultOptions = Options<Defaults>;

template <class Options> class Build;
} // namespace VOLTISO_NAMESPACE::allocator::splay

//

namespace VOLTISO_NAMESPACE::allocator::splay::_ {
template <size_t PAGE_SIZE> auto roundUpToFullPages(size_t numBytes) {
  return (numBytes + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);
  //
}
/** @internal used to self-allocate memory for our internal structures, in order
 * to avoid dependency on self */
template <class Options> class SelfAllocator : public Object<Options> {
public:
  using Handle = void *;

public:
  void *allocateBytes(size_t numBytes) {
    // LOG(INFO) << "SelfAllocator::allocateBytes(" << numBytes << ")";
    DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);

    if (this->spareBlocks.numItems &&
        this->spareBlocks.first().numBytes >= numBytes) { // ! likely? unlikely?
      // LOG(INFO) << "USE SPARE BLOCK DIRECTLY";
      if (this->spareBlocks.first().numBytes != numBytes) [[likely]] {
        auto ptr = this->spareBlocks.first().ptr;
        reinterpret_cast<std::uintptr_t &>(this->spareBlocks.first().ptr) +=
            numBytes;
        this->spareBlocks.first().numBytes -= numBytes;
        return ptr;
      } else [[unlikely]] {
        auto ptr = this->spareBlocks.first().ptr;
        this->spareBlocks.pop();
        return ptr;
      }
    }

    auto hNode = _alloc()
                     .splay(_alloc().root)
                     .lowerBound(numBytes << (sizeof(std::size_t) * 8 / 2));

    if (hNode) [[likely]] {
      auto entry = _alloc().splay(_alloc().root).erase(hNode);
      DCHECK_GE(entry.numBytes, numBytes);
      // LOG(INFO) << "begins[" << entry.memory.value << "].erase()";
      _alloc().begins[entry.memory.value].erase();
      _alloc().ends[entry.memory.value + entry.numBytes].erase();
      this->_add(entry.memory.value + numBytes, entry.numBytes - numBytes);
      return entry.memory;
    } else [[unlikely]] {
      // LOG(INFO) << "SelfAllocator: allocate full page";
      auto newNumBytes = roundUpToFullPages<Options::PAGE_SIZE>(numBytes);
      auto memory = Options::Allocator::instance().allocateBytes(newNumBytes);
      this->_add(memory.value + numBytes, newNumBytes - numBytes);
      return memory;
    }
  }

public:
  void *reallocateBytes(void *handle, size_t numBytes, size_t newNumBytes) {
    // LOG(INFO) << "SelfAllocator::reallocateBytes(" << handle << ", " <<
    // numBytes
    //           << ", " << newNumBytes << ")";

    // don't optimize for this branch; shrinking allocations is not done when
    // time is critical
    if (newNumBytes <= numBytes) [[unlikely]] {
      auto pLeftover = (std::uintptr_t)handle + newNumBytes;
      if (newNumBytes != numBytes) [[likely]] {
        auto leftoverSize = numBytes - newNumBytes;
        _add(pLeftover, leftoverSize);
      }
      return handle;
    }

    // auto pAdditional = (std::uintptr_t)handle + numBytes;
    // auto aAdditionalBegin = _alloc().begins[pAdditional];

    // // optimize for fast in-place realloc
    // if (aAdditionalBegin.exists) [[likely]] {
    //   auto aNode = _alloc().nodes[aAdditionalBegin];
    //   auto numAdditionalBytes = newNumBytes - numBytes;
    //   if (aNode->entry.numBytes >= numAdditionalBytes) [[likely]] {
    //     // realloc in place possible
    //     auto entry = _alloc().splay(_alloc().root).erase(aNode);

    //     // remove from begins and ends
    //     aAdditionalBegin.erase();
    //     _alloc().ends[entry.memory.value + entry.numBytes].erase();

    //     // shrink left numAdditionalBytes
    //     if (aNode->entry.numBytes != numAdditionalBytes) [[likely]] {
    //       DCHECK_GT(aNode->entry.numBytes, numAdditionalBytes);
    //       auto pLeftover = (std::uintptr_t)handle + newNumBytes;
    //       auto leftoverSize = aNode->entry.numBytes - numAdditionalBytes;
    //       _add(pLeftover, leftoverSize);
    //     } else [[unlikely]] {
    //       // whole block consumed
    //     }
    //     return handle;
    //   }
    // }

    auto newHandle = allocateBytes(newNumBytes);
    ::memcpy(newHandle, handle, numBytes);
    this->freeBytes(handle, numBytes);
    return newHandle;
  }

public:
  void freeBytes(void *ptr, size_t numBytes) {
    // LOG(INFO) << "SelfAllocator::freeBytes(" << ptr << ", " << numBytes <<
    // ")";
    this->_add(ptr, numBytes);
  }

public:
  void *operator()(void *ptr) { return ptr; }

public:
  // friend singleton;
  // friend singleton::perThread;
  SelfAllocator() = default;

private:
  friend Build<Options>;
  static constexpr auto &instance() {
    // todo: store in SplayAllocator
    return singleton::instance<SelfAllocator>();
    // return singleton::perThread::instance<SelfAllocator>();
  }

private:
  auto &_alloc() {
    // return context::get<SelfAllocator>();
    return Build<Options>::instance();
  }

private:
  struct SpareBlock {
    void *ptr;
    size_t numBytes;
  };
  using SpareBlocks = DynamicArray<SpareBlock>::template IN_PLACE_ONLY_<100>;
  SpareBlocks spareBlocks;

  void _add(std::uintptr_t memory, size_t numBytes) {
    _add(reinterpret_cast<void *>(memory), numBytes);
  }

  void _add(void *memory, size_t numBytes) {
    // LOG(INFO) << "Adding spare block " << numBytes << " bytes at " << memory;
    DCHECK_LT(spareBlocks.numItems, spareBlocks.NUM_SLOTS);
    spareBlocks.push({memory, numBytes});
  }

  void _cleanup() {
    // if (spareBlocks.numItems) {
    //   LOG(INFO) << "Cleaning up " << spareBlocks.numItems << " spare blocks";
    // }
    while (spareBlocks.numItems) [[unlikely]] {
      auto block = spareBlocks.pop();
      // LOG(INFO) << "Registering " << block.numBytes << " spare bytes at "
      //           << block.ptr;
      _alloc().freeBytes(typename Build<Options>::Handle(block.ptr),
                         block.numBytes);
    }
  }
}; // class SelfAllocator

template <class Options> struct SplayEntry {
  const std::size_t key;
  const std::size_t numBytes;
  const Options::Allocator::Handle memory;

  SplayEntry(std::size_t numBytes, Options::Allocator::Handle memory)
      : key(_key(numBytes, memory)), numBytes(numBytes), memory(memory) {}

  SplayEntry(const SplayEntry &) = default;

  SplayEntry &operator=(const SplayEntry &other) {
    static_assert(std::is_trivially_destructible_v<SplayEntry>);
    new (this) SplayEntry(other);
    return *this;
  }

  auto &value() const { return memory; }

private:
  static constexpr auto _key(std::size_t numBytes,
                             Options::Allocator::Handle memory) {
    constexpr auto BITS = sizeof(std::size_t) * 8 / 2;
    constexpr auto MASK = (std::size_t(1) << BITS) - 1;
    DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);
    DCHECK_EQ(memory.value % alignof(std::max_align_t), 0);
    auto key = numBytes / alignof(std::max_align_t);
    DCHECK_LE(key, MASK);
    auto memoryHash = (memory.value & MASK) / alignof(std::max_align_t);
    auto hash = key ^ memoryHash;
    auto result = (key << BITS) | hash;
    return result;
  }
};

template <class Options>
using Pool = Pool<int>::template Allocator_<SelfAllocator<Options>>;

template <class Options> struct SplayNode {
  SplayEntry<Options> entry;
  using Handle = typename Pool<Options>::Handle::template Brand_<void>;
  Array<Handle, 2> children = {Handle::null, Handle::null};

  // SplayNode &operator=(const SplayNode &other) {
  //   memcpy(this, &other, sizeof(SplayNode));
  //   return *this;
  // }
};
} // namespace VOLTISO_NAMESPACE::allocator::splay::_

namespace std {
template <class Options>
ostream &operator<<(
    ostream &os,
    const VOLTISO_NAMESPACE::allocator::splay::_::SplayEntry<Options> &entry) {
  return os << "{numBytes: " << entry.numBytes << ", memory: " << entry.memory
            << "}";
}

template <class Options>
ostream &operator<<(
    ostream &os,
    const VOLTISO_NAMESPACE::allocator::splay::_::SplayNode<Options> &node) {
  return os << "{entry: " << node.entry << ", children: " << node.children
            << "}";
}
} // namespace std

//

namespace VOLTISO_NAMESPACE::allocator::splay {
template <class _Options> class Build : public Object<_Options> {
private:
  using Self = Build;

public:
  using Options = _Options;
  // parent allocator
  using Allocator = Options::Allocator;
  static constexpr auto PAGE_SIZE = Options::PAGE_SIZE;
  using Handle = Allocator::Handle ::template Brand_<Self>;

private:
  friend _::SelfAllocator<Options>;
  using AllocatorHandle = Allocator::Handle;

  context::Guard<_::SelfAllocator<Options>> splaySelfGuard =
      context::Guard(_::SelfAllocator<Options>::instance());

private:
  using Pool = _::Pool<Options>;
  using SplayEntry = _::SplayEntry<Options>;
  using SplayNode = _::SplayNode<Options>;
  using Nodes = Pool ::template Item_<SplayNode>;
  Nodes nodes;
  Nodes::Handle root = Nodes::Handle::null;

private:
  Splay<Nodes> splay = Splay<Nodes>(this->nodes);

  static constexpr size_t BUCKET_IN_PLACE = 1;
  using Edges =
      HashMap<std::uintptr_t, typename Nodes::Handle>::template Allocator_<
          _::SelfAllocator<Options>>;
  // ::template BUCKET_IN_PLACE_<
  //     BUCKET_IN_PLACE>;
  Edges begins, ends;

private:
  friend singleton;
  // friend singleton::perThread;
  Build() = default;

public:
  static constexpr auto &instance() {
    // return context::get<typename Options::Final>();
    return singleton::instance<Self>();
    // return singleton::perThread::instance<Self>();
  }

public:
  Handle allocateBytes(const size_t numBytes) {
    // LOG(INFO) << "Build::allocateBytes(" << numBytes << ")";
    DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);
    // numBytes = divCeil(numBytes, alignof(std::max_align_t)) *
    //            alignof(std::max_align_t);
    auto hNode = splay(root).lowerBound(numBytes);

    if (!hNode) [[unlikely]] {
      auto newNumBytes = _::roundUpToFullPages<Options::PAGE_SIZE>(numBytes);
      auto memory = Allocator::instance().allocateBytes(newNumBytes);
      _add(memory, newNumBytes);
      hNode = splay(root).lowerBound(numBytes);
    }

    auto aNode = nodes[hNode];
    auto result = aNode->entry.memory;
    DCHECK_GE(aNode->entry.numBytes, numBytes);
    if (aNode->entry.numBytes > numBytes) [[likely]] {
      // Split the node
      begins[aNode->entry.memory.value].erase();

      splay(root).unlink(hNode);
      aNode->entry = {aNode->entry.numBytes - numBytes,
                      AllocatorHandle(aNode->entry.memory.value + numBytes)};
      splay(root).link(hNode);

      begins[aNode->entry.memory.value].insert(hNode);
    } else [[unlikely]] {
      DCHECK_EQ(aNode->entry.numBytes, numBytes);
      begins[aNode->entry.memory.value].erase();
      ends[aNode->entry.memory.value + aNode->entry.numBytes].erase();
      splay(root).erase(hNode);
    }

    DCHECK_EQ(_::SelfAllocator<Options>::instance().spareBlocks.numItems, 0);

    return Handle(result.value);
  }

  Handle reallocateBytes(const Handle &handle, size_t numBytes,
                         size_t newNumBytes) {
    // LOG(INFO) << "Build::reallocateBytes(" << handle << ", " << numBytes <<
    // ", "
    //           << newNumBytes << ")";
    // don't optimize for this branch; shrinking allocations is not done when
    // time is critical
    if (newNumBytes <= numBytes) [[unlikely]] {
      auto pLeftover = (std::uintptr_t)handle.value + newNumBytes;
      if (newNumBytes != numBytes) [[likely]] {
        auto leftoverSize = numBytes - newNumBytes;
        _add(AllocatorHandle(pLeftover), leftoverSize);
      }
      DCHECK_EQ(_::SelfAllocator<Options>::instance().spareBlocks.numItems, 0);
      return handle;
    }

    auto pAdditional = (std::uintptr_t)handle.value + numBytes;
    auto &aAdditionalBegin = begins[pAdditional];

    // optimize for fast in-place realloc
    if (aAdditionalBegin.exists) [[likely]] {
      auto aNode = nodes[aAdditionalBegin];
      auto numAdditionalBytes = newNumBytes - numBytes;
      if (aNode->entry.numBytes >= numAdditionalBytes) [[likely]] {
        // shrink left numAdditionalBytes
        aAdditionalBegin.erase(); // remove from begins
        auto pLeftover = (std::uintptr_t)handle.value + newNumBytes;
        if (aNode->entry.numBytes != numAdditionalBytes) [[likely]] {
          DCHECK_GT(aNode->entry.numBytes, numAdditionalBytes);
          begins[pLeftover].insert(aNode);
          splay[root].unlink(aNode);
          aNode->entry.numBytes -= numAdditionalBytes;
          splay[root].link(aNode);
        } else [[unlikely]] {
          // whole block consumed
          splay[root].erase(aNode);
        }
        DCHECK_EQ(_::SelfAllocator<Options>::instance().spareBlocks.numItems,
                  0);
        return handle;
      }
    }

    auto newHandle = allocateBytes(newNumBytes);
    ::memcpy(newHandle, handle, numBytes);
    this->freeBytes(handle, numBytes);
    DCHECK_EQ(_::SelfAllocator<Options>::instance().spareBlocks.numItems, 0);
    return newHandle;
  }

  void freeBytes(const Handle &handle, size_t numBytes) {
    // LOG(INFO) << "Build::freeBytes(" << handle << ", " << numBytes << ")";
    DCHECK_GT(numBytes, 0);
    _add(AllocatorHandle(handle.value), numBytes);
    DCHECK_EQ(_::SelfAllocator<Options>::instance().spareBlocks.numItems, 0);
  }

  void *operator()(const Handle &handle) { return handle; }

  const void *operator()(const Handle &handle) const {
    return const_cast<Self &>(*this)(handle);
  }

private:
  // add new block of memory, merging with adjacent blocks if possible
  // adds at most 1 entry to `begins`
  // adds at most 1 entry to `ends`
  // adds at most 1 entry to `nodes`
  void _add(Allocator::Handle memory, size_t numBytes) {
    DCHECK_NE(memory.value, 0);
    DCHECK_GT(numBytes, 0);
    DCHECK_EQ(numBytes % alignof(std::max_align_t), 0);

    // _selfGrow();

    auto pNewBlock = (std::uintptr_t)memory.value;
    auto aPrevEnd = ends[pNewBlock];
    if (aPrevEnd.exists) [[unlikely]] {
      // have previous block
      auto aPrevNode = nodes[aPrevEnd];
      aPrevEnd.erase();
      // LOG(INFO) << splay(root);
      splay(root).unlink(aPrevNode);
      auto prevNodeNumBytes = aPrevNode->entry.numBytes;
      prevNodeNumBytes += numBytes;
      auto pNewBlockEnd = (std::uintptr_t)memory.value + numBytes;
      auto aNextBegin = begins[pNewBlockEnd];
      if (aNextBegin.exists) [[unlikely]] {
        // also have next block
        auto aNextNode = nodes[aNextBegin];
        prevNodeNumBytes += aNextNode->entry.numBytes;
        splay(root).erase(aNextNode);
        aNextBegin.erase();
      } else [[likely]] {
        // have previous, but no next block
        ends[pNewBlockEnd].insert(aPrevNode);
      }
      aPrevNode->entry = {prevNodeNumBytes, aPrevNode->entry.memory};
      splay(root).link(aPrevNode);
    } else [[likely]] {
      // no previous block
      auto pNewBlockEnd = (std::uintptr_t)memory.value + numBytes;
      auto aNextBegin = begins[pNewBlockEnd];
      if (aNextBegin.exists) [[unlikely]] {
        // have next block
        auto aNextNode = nodes[aNextBegin];
        begins[aNextNode->entry.memory.value].erase();
        splay(root).unlink(aNextNode);
        aNextNode->entry = {aNextNode->entry.numBytes + numBytes, memory};
        splay(root).link(aNextNode);
        begins[pNewBlock].insert(aNextNode);
      } else [[likely]] {
        // no previous, no next block
        auto aNode =
            this->nodes.insert(SplayNode{SplayEntry{numBytes, memory}});
        // auto newNode = splay(root).insert(SplayEntry{numBytes, memory});
        // LOG(INFO) << "begins[" << pNewBlock << "].insert(" << aNode << ")";
        ends[pNewBlockEnd].insert(aNode); // has to be first
        begins[pNewBlock].insert(aNode);
        splay(root).link(aNode);
      }
    }
    _::SelfAllocator<Options>::instance()._cleanup();
  } // _add
}; // class Build
} // namespace VOLTISO_NAMESPACE::allocator::splay
VOLTISO_OBJECT_FINAL(allocator::splay)

namespace VOLTISO_NAMESPACE::allocator {
using Splay = splay::Final<splay::DefaultOptions>;

inline auto g_splayGuard = v::context::Guard(v::allocator::Splay::instance());

} // namespace VOLTISO_NAMESPACE::allocator
